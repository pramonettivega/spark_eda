{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1d52d9-d836-4699-b174-2f50f8f481b9",
   "metadata": {},
   "source": [
    "# Spark - Exploratory Data Analysis\n",
    "\n",
    "In this courselet, we will explore the use of Spark as a Data Science tool when working with Big Data. By the end of this courselet, you would be able to:\n",
    "\n",
    "- Perform a descriptive analysis\n",
    "- Compare the performance of Spark under different data formats (csv vs. parquet)\n",
    "- Identify the advantages of using Spark in a cluster, by relying on different optimization strategies.\n",
    "\n",
    "## What is Spark?\n",
    "\n",
    "[Apache Spark](https://spark.apache.org/) is an open-source engine used for the processing and analysis of large-scale data. Today, Spark is one of the most popular frameworks for the developement of data engineering, data science and machine learning tasks.\n",
    "\n",
    "## Advantages of Spark\n",
    "\n",
    "- Multi-language: It can be implemented through multiple-languages (Python, R, Java, Scala), making it flexible for diverse users\n",
    "- Speed: Spark leverages on RAM to perform it's computational tasks, which translates into faster data processing\n",
    "- Parallelism and fault tolerance: Spark is designed to be used in the context of a multi-node cluster. Therefore, its design aims for a parallel data processing across nodes, as well as fault tolerance in the case that a node presents a failure\n",
    "- Wide community of users: Given it's popularity, there is a huge open-source community, which allows Spark to be constantly developed and improved\n",
    "\n",
    "## Disadvantages of Spark\n",
    "\n",
    "- Memory consumption: Given that Spark uses RAM to optimize speed in processing, this can also lead to several instances of *OutOfMemory* error if not managed properly \n",
    "- Manual fine-tuning and optimization: To optimize the overall performance of Spark, the user must manually [tune the settings](https://www.databricks.com/glossary/spark-tuning) of the cluster in terms of hardware. This process is not straightforward and a inadequate setting up might result in a non-optimal perfomance\n",
    "- Inneficient management of small files: For cases of small datafiles, Spark is not an optimal framework, given the high transaction cost of distributing and collecting data across nodes. For the case of small datafiles, using Pandas or a more conventional approach is more efficient\n",
    "\n",
    "## Importing libraries and starting our Spark session.\n",
    "\n",
    "To start working, we are first going to import our main library and initialize both our [SparkSession](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession) and our [SparkContext](https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968789c-82a1-4388-a223-f272fdd686be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Starting our Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EDA Spark\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Starting our Spark Context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c950710-0837-4333-a2ce-26b2efb9b537",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "The [**Resilient Distributed Dataset (RDD)**](https://spark.apache.org/docs/latest/rdd-programming-guide.html) is the main data structure in Spark. It consists of a collection of object distributed across the nodes of the cluster. This allows for the processes to be parallelized across the nodes, and to guarantee fault tolerance. RDD's posses two fundamental types of operations: [**transformations**](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations),  which are operations that convert RDD's into new RDD's, and [**actions**](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions), which return non-RDD values/objects.\n",
    "\n",
    "A fundamental piece of Spark's design is the concept of [**lazy evaluation**](https://towardsdatascience.com/3-reasons-why-sparks-lazy-evaluation-is-useful-ed06e27360c4). This concept refers to an execution plan in which Spark does not perform a computational task until explicitly intructred to do so. When we perform a transformation and create a new RDD, Spark does not perform any computation at that point. Instead, what it creates is an execution plan. It is not until we perform an action, when Spark actually executes the task and returns a new object. In this way, the user can perform as many transformations as desired, but non of them will be executed until an action is performed, achieving a more efficient use of the resources.\n",
    "\n",
    "Let's now work with an example RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982051a-5fc4-4585-b1b3-91284c2e35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a list of values\n",
    "lst = [i for i in range(1000000)]\n",
    "# Executing the function \"parallelize\", converts our list into an RDD\n",
    "rdd = sc.parallelize(lst)\n",
    "rdd # As we will see, the representation of the RDD is not a list anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ac0a8-9b88-4e51-820e-ebb2f6690198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a transformation. \n",
    "squared_rdd = rdd.map(lambda x: x**2) # Transforming our RDD into a new one with the values squared\n",
    "squared_rdd # As we stated previously, the previous transformation created a new RDD, but no computation has been performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4311726-4abb-41e6-ba86-2b3e437461b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_rdd = squared_rdd.collect() # Performing and action to collect our RDD into a list again\n",
    "squared_rdd[:16] # Now we have a list of squared values. We only show the first 16 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c721b3b-b3f1-4bb6-be60-c670573c3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another transformation\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0) # Filtering our RDD into a new one with even numbers. This is a transformation\n",
    "filtered_rdd.count() # Returning the count of even numbers. This is our action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ccb2d-5c6b-4ddf-940f-9a09e4b720a9",
   "metadata": {},
   "source": [
    "The previous example showed a series of tasks over a list which was transformed into an RDD. However, the creation of RDD's is not limited to lists. Multiple data sources can be transformed into RDD's to take advantage of the distribution principle. In the next cells of code, we are going to perform a very common task, which is counting the frequencies of words in a given text. For that, we are going to do the following:\n",
    "\n",
    "1. Define a stopwords list\n",
    "2. Load our shakespeare.txt file, which is a compilation with all the works from William Shakespeare (a big text file)\n",
    "3. Perform a series of transformations, before calling an action and returning a list of tuples with the 10 most frequent words in Shakespeare works, and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3b368-5b79-4b73-8e8b-212342c1fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of stopwords used by the nltk library. We defined it here to save time installing the library.\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', \n",
    "'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    " 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', \n",
    "'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
    "'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n",
    "'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "'s', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',\n",
    " 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \n",
    "'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    " \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", \n",
    "'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beac4a3-5df9-40d3-8410-b9ef12ef71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare = spark.read.text(\"data/shakespeare.txt\") \n",
    "shakespeare # By using the read.text function, our txt file is read as a DataFrame. We will explore further on DataFrames after this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b820c-84ad-4fd4-b26a-9fba9ffc7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = (\n",
    "    shakespeare\n",
    "    .rdd.flatMap(lambda line: line[0].split(\" \")) # First, we convert the DataFrame into an RDD, we \"flatten\" the elements of the RDD and split them by space. Basically, we make a list of all the words\n",
    "    .filter(lambda word: word.lower() not in stop_words and word != \"\") # Now we filter each word lowered, removing stop words and empty strings \n",
    "    .map(lambda word: (word, 1)) # For each word, we create a tuple with the word (which represents the key) and a value of 1 as a count. This tuple represents a Key-Value pair (K,V)\n",
    "    .reduceByKey(lambda x, y: x + y) #For each Key, we perform the specified function on it's values. In this case, it is a sum of the Values with the same Key (x+y)\n",
    "    .sortBy(lambda x: x[1], ascending=False) # We sort the tuples by the count, in descending order\n",
    "    .take(10) # Finally, we perform a \"take\" action and return the first 10 tuples\n",
    ")\n",
    "\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02fcc11-3a0f-4533-8b01-74880fb6848d",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "Another essential data structure in Spark, particularly for data analysis, are DataFrames, or Distributed DataFrames. This type of DataFrame is built on top of RDD's, but with the advantage of having a better defined structure, providing a schema and facilitating data interaction and query execution. It also follows Spark's principle of distributed data processing across the different nodes.\n",
    "\n",
    "For this courselet, we are exploring the taxi trips reported to the City of Chicago in 2020. This data is publicly available through the [Chicago Data Portal](https://data.cityofchicago.org/en/Transportation/Taxi-Trips-2020/r2u4-wwk3/about_data). \n",
    "\n",
    "We will proceed to load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59370e32-2159-40db-aa4e-1ec93b84e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/chicago-taxi-2020.csv\",header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4801a22-cbcf-42de-a78d-3bb97ce4e473",
   "metadata": {},
   "source": [
    "As the our first actions, we can print the schema of the data, and count the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe3471-4116-4b16-9c5e-715b4a49ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f1714-17e2-4f1c-82cc-7850a00cae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23273e0-6cfb-4239-9ae4-7cfc06b49db8",
   "metadata": {},
   "source": [
    "## DataFrames Operations\n",
    "\n",
    "In the next cells of codes, we are going to explore some basic ways to manipulate our DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404dccd-2e9f-4251-a7be-eac409016957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can take a subset of the data\n",
    "df.select('Trip ID', 'Trip Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84763ee2-3ac3-488b-a406-b4673d28abd5",
   "metadata": {},
   "source": [
    "As it happens with RDD's, dataframes also follow the principle of lazy evaluation. In this case, when we called for a subsetting of the dataframe, Spark only returned a logical representation of it. If we want to actually display values, we must perform an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77c520-4db3-4a30-8b08-79132d79afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Trip ID', 'Trip Total').show(10) # We are going to show the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea61eda-3457-486d-af0d-f8026b0d93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply filters to our df's columns\n",
    "df.filter(df[\"Tips\"] > 0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dea2f1-c1cc-421e-b599-affcfaf8f104",
   "metadata": {},
   "source": [
    "Results don't look good right? They are not readable at all. Maybe we should perform a different type of action method. \n",
    "\n",
    "The *.toPandas()* method allows us to convert an RDD into a Pandas dataframe. It is especially recommended if we expect the output of our query to be small. Let's use that to show the first 5 observations of our filtered df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb05efe-45b5-4ea9-8c7a-406185c763f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"Tips\"] > 0).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adab33d-246a-4dc3-84d1-59e5e17febf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated rows\n",
    "df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a487c4c-ee01-4b63-b732-d9854765383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort df by a column\n",
    "df.orderBy(df[\"Fare\"].desc()).limit(5).select('Trip ID', 'Fare').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf42f3-4cc5-4c69-a42c-8bc98f51ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and create new columns - Using \\ allows us to continue our transformations sequentially while ordering our code better\n",
    "\n",
    "df.withColumn(\"Fare in Euros\", df[\"Fare\"] * 0.92) \\\n",
    "  .orderBy(df[\"Fare\"].desc()) \\\n",
    "  .limit(5) \\\n",
    "  .select('Trip ID', 'Fare in Euros') \\\n",
    "  .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c83cf6-8a34-49e5-b5d8-9c06fe1b79f4",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "To support our operations, we are going to use the collections of functions provided by pyspark.sql. You can consult the full list in the [source documentation.](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.max.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63ade3-208e-4d09-8248-11ab0035e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1b324-6d46-4e82-a6ac-54fdc2d0d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of a column\n",
    "df.select(F.mean(\"Trip Total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7fa356-08d8-4f0a-b8d0-74433b8b0afb",
   "metadata": {},
   "source": [
    "As you can observe, our previous line of code has not returned the mean yet. Instead, it returned a DataFrame object. We must perform an action to display the mean. The following line of code extends the previous one with the extension of the *show.()* action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d835b-6929-427f-8ae8-801357a9382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(F.mean(\"Trip Total\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d2064-e578-4171-86ae-bfaf52acb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of multiple columns\n",
    "df.select(F.mean(\"Trip Total\"), F.mean(\"Trip Seconds\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc5745-4f87-4f84-b6f8-bfcee9abe39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max value of a column\n",
    "df.select(F.max(\"Trip Total\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2350f-6b33-4416-b94e-3dd0a45bff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average trip total by community area.\n",
    "df.groupBy(\"Pickup Community Area\") \\\n",
    "    .agg(F.avg(\"Trip Total\")\n",
    "    .alias(\"AVG Trip Total\")) \\\n",
    "    .na.drop(subset=[\"Pickup Community Area\"]) \\\n",
    "    .orderBy(F.col(\"Pickup Community Area\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0edcb0-987c-4370-a4db-73c059d1df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the missingness rate per column\n",
    "missingness = df.agg(*[(1 - (F.count(F.col(c)) / F.count('*')))\n",
    "         .alias(c + '_missing_rate') for c in df.columns])\n",
    "missingness.toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b3e21d-2852-47b1-b00e-737549c9016e",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "One of the main features in Spark is the incorporation of an SQL module which allows us to analyze strcutured data through the use of SQL Queries. Let's take a look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bdac0-f75e-4097-856c-432df5e33b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"trips\") # We create this temporary view so Spark recognizes our DataFrame as a table when writing queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a8190-5aee-43ac-883d-9fe68b4b7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Taxi ID that charged the most expensive trip\n",
    "spark.sql(\"\"\"\n",
    "    SELECT `Taxi ID`\n",
    "    FROM trips\n",
    "    WHERE `Trip Total` = (SELECT MAX(`Trip Total`) FROM trips)\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c531a-b9ee-4e58-8708-82ab5c8cca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 5 companies with the highest avg fares\n",
    "spark.sql(\"\"\"\n",
    "    SELECT company, AVG(Fare) as avg_fare\n",
    "    FROM trips\n",
    "    GROUP BY Company\n",
    "    ORDER BY avg_fare DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3e4bd-f692-4bc5-9ac3-a62a1b88fedd",
   "metadata": {},
   "source": [
    "## Visualizing Data\n",
    "\n",
    "Spark does not count with a dedicated library or API for data visualization. However, we can generate non-RDD objects and plot them through the use of conventional python libraries like Matpolib or Seaborn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629f80b-ea1c-4f92-aecd-78b36472a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# We create a list of for each column to visualize.\n",
    "trip_seconds = df.select('Trip Seconds').rdd.flatMap(lambda x: x).collect() \n",
    "trip_total = df.select('Trip Total').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "sns.scatterplot(x=trip_seconds, y=trip_total)\n",
    "plt.title('Scatter Plot of Trip Seconds vs Trip Total')\n",
    "plt.xlabel('Trip Seconds')\n",
    "plt.ylabel('Trip Total')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a36c4-7fda-4bfd-b2e4-79e9979056e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = spark.sql(\"\"\"\n",
    "    SELECT company, AVG(`Trip Miles`) as avg_trip\n",
    "    FROM trips\n",
    "    GROUP BY Company\n",
    "    ORDER BY avg_trip DESC\n",
    "    LIMIT 5\n",
    "\"\"\").toPandas() # Another approach is to query our table, and create a simple PandasDF from it to plot it.\n",
    "\n",
    "sns.barplot(x='company', y='avg_trip', data=pandas_df)\n",
    "plt.title('Top 5 Companies with the longest AVG Trips')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf757f2-3664-40af-9edf-904d275ef5e0",
   "metadata": {},
   "source": [
    "## CSV vs Parquet\n",
    "\n",
    "So far, we have returned some simple results using the data in csv format. However, in some cases, data might be available for our convenience in the [Parquet file format](https://parquet.apache.org/#td-block-1). Given it's features and design, this open-source format allows Spark to make faster reads to the data, and tends to have smaller file sizes compared to csv. As we have our taxis data also in parquet format, we are going to load a new DataFrame using this file format, and compare the execution time between the csv DataFrame and the Parquet DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0cc951-8f18-4e3e-9344-9b1c49c49c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(\"data/chicago-taxi-2020.parquet\",header=True, inferSchema=True)\n",
    "df_parquet.printSchema() # We confirm the schema is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32cffd-ff29-4e1f-bfbc-158e1b524390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# For this comparison, we are executing our previous query of calculating the avg fare grouped by Pickup Community Area\n",
    "\n",
    "# CSV Performance\n",
    "start_csv = time.time()\n",
    "df.groupBy(\"Pickup Community Area\") \\\n",
    "    .agg(F.avg(\"Trip Total\") \\\n",
    "    .alias(\"AVG Trip Total\")) \\\n",
    "    .na.drop(subset=[\"Pickup Community Area\"])\\\n",
    "    .orderBy(F.col('Pickup Community Area')).show()\n",
    "end_csv = time.time()\n",
    "total_csv = end_csv - start_csv\n",
    "print(f\"Execution Time (CSV): {total_csv}\\n\")\n",
    "\n",
    "# Parquet Performance\n",
    "start_parquet = time.time()\n",
    "df_parquet.groupBy(\"Pickup Community Area\") \\\n",
    "    .agg(F.avg(\"Trip Total\") \\\n",
    "    .alias(\"AVG Trip Total\")) \\\n",
    "    .na.drop(subset=[\"Pickup Community Area\"])\\\n",
    "    .orderBy(F.col('Pickup Community Area')).show()\n",
    "end_parquet = time.time()\n",
    "total_parquet = end_parquet - start_parquet\n",
    "print(f\"Execution Time (Parquet): {total_parquet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2da3be-8cdf-48bf-8314-1ae6b8228fe9",
   "metadata": {},
   "source": [
    "As we see, in this first example, using a parquet format resulted in a faster execution, providing the same results. Let's look at a second example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98d6b9-bed6-4daf-8773-7662b5eae80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more simple example, returning the avg trip total and the avg trip duration\n",
    "\n",
    "# CSV Performance\n",
    "start_csv = time.time()\n",
    "df.select(F.mean(\"Trip Total\"), F.mean(\"Trip Seconds\")).show()\n",
    "end_csv = time.time()\n",
    "total_csv = end_csv - start_csv\n",
    "print(f\"Execution Time (CSV): {total_csv}\\n\")\n",
    "\n",
    "# Parquet Performance\n",
    "start_parquet = time.time()\n",
    "df_parquet.select(F.mean(\"Trip Total\"), F.mean(\"Trip Seconds\")).show()\n",
    "end_parquet = time.time()\n",
    "total_parquet = end_parquet - start_parquet\n",
    "print(f\"Execution Time (CSV): {total_parquet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec044ad-16e3-4a0b-92c5-2502aaa473bb",
   "metadata": {},
   "source": [
    "## Fine Tuning and Improving Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3fbda-da46-4b6f-b12e-e949c2b50dbc",
   "metadata": {},
   "source": [
    "To take the most advantage of Spark and improve the overall performance of our cluster, it is important to understand and determine the right parameter configuration in our environment.\n",
    "\n",
    "The following are a series of features and techniques that can enhance Spark's overall performance.\n",
    "\n",
    "**Right number of partitions**\n",
    "\n",
    "Identifying the right number of partitions is key to achieve a solid performance of our computational tasks. A partition refers to a portion of the data distributed among the cores. \n",
    "\n",
    "To change the number of partitions in our RDD, Spark possesses [two different functions](https://sparkbyexamples.com/spark/spark-repartition-vs-coalesce/):\n",
    "\n",
    "- [*.repartition()*](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.repartition.html): Can be used to either increase or decrease the number of partitions by shuffling the data and redistributing it in *n* partitions.\n",
    "- [*.coalesce()*](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html): Can only be used to decrease the number of partitions without shuffling the data. For some contexts, this is a more efficient way to decrease the number of partions.\n",
    "\n",
    "It is important to take into consideration that partitioning is an expensive task, and therefore, increasing the number of partitions in our data does not necessarily represents an improvement in performance. \n",
    "\n",
    "To start our fine-tuning, let's get the current number of partitions in our Parquet DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06061bb-e981-4c94-8c2d-d0925d75daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81705467-5a21-44c3-93f1-fcb41e1000ad",
   "metadata": {},
   "source": [
    "We can repartition our DataFrame and see if we achieve a better performance. In the following cell of code, we will iterate over different partition sizes, repeat our previous grouping task, and identify which size is the optimal given our current resources and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa38fe-511e-4cf3-b855-8bb551198735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_time(ddf, partitions):\n",
    "    ddf_repartitioned = ddf.repartition(partitions) # Repartition the RDD\n",
    "    start_time = time.time()\n",
    "    ddf_repartitioned.groupBy(\"Pickup Community Area\") \\\n",
    "    .agg(F.avg(\"Trip Total\") \\\n",
    "    .alias(\"AVG Trip Total\")) \\\n",
    "    .na.drop(subset=[\"Pickup Community Area\"])\\\n",
    "    .orderBy(F.col('Pickup Community Area'))\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "partitions = [5, 10, 15, 20,\n",
    "              25, 30, 35, 40, \n",
    "              45, 50]\n",
    "\n",
    "times = [task_time(df_parquet, p) for p in partitions]\n",
    "\n",
    "optimal_partitions = partitions[times.index(min(times))] # We identify the partition size with the lowest time\n",
    "\n",
    "print(f\"The optimal number of partitions is {optimal_partitions} with a time of {min(times)} seconds\\n\")\n",
    "\n",
    "# Plotting\n",
    "plt.plot(partitions, times)\n",
    "plt.xlabel('Number of Partitions')\n",
    "plt.ylabel('Time taken (seconds)')\n",
    "plt.title('Effect of Partitioning on Performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c5fea-bcdb-4b7a-bf39-3c640ae4bef5",
   "metadata": {},
   "source": [
    "As we can observe in the graph, an initial increasing in the number of partitions improves the computation time. However, as the number of partitions increases, the repartitioning does not offer an improval in performance.\n",
    "\n",
    "**Data Persistence**\n",
    "\n",
    "Another way to improve our performance is by persisting our recurrently used RDD's of Dataframes. By persisting our data, we avoid the task of recomputing the data from the source everytime an action is called. There are different [persistence levels](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence) available for set-up and the choice mostly depends on the size and type of data we are working with. To persist our data, we have two different methods:\n",
    "\n",
    "-[*.persist()*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.spark.persist.html): With this method, the user defines the storage level to persist the data.\n",
    "\n",
    "-[*cache()*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.cache.html): With this method, data is cached with the default level *MEMORY_AND_DISK*.\n",
    "\n",
    "Let's make a comparison of the performance of a query in our parquet DataFrame before and after caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bf73a-4969-4daf-b6e3-1af07dc60468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-caching\n",
    "start_pre = time.time()\n",
    "df_parquet.groupBy(\"Pickup Community Area\") \\\n",
    "    .agg(F.avg(\"Trip Total\") \\\n",
    "    .alias(\"AVG Trip Total\")) \\\n",
    "    .na.drop(subset=[\"Pickup Community Area\"])\\\n",
    "    .orderBy(F.col('Pickup Community Area')).show()\n",
    "end_pre = time.time()\n",
    "total_pre = end_pre - start_pre\n",
    "print(f\"Is the dataframe cached: {df_parquet.is_cached}\")\n",
    "print(f\"Execution Time (Pre-cached): {total_pre}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc522c-00c0-4269-b8e3-4d09e954a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.cache()\n",
    "df_parquet.take(1) #We need to perform a first action to actually cache the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e18188-1550-4648-a20d-fb3cf1d5b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached\n",
    "start_post = time.time()\n",
    "df_parquet.groupBy(\"Pickup Community Area\") \\\n",
    "    .agg(F.avg(\"Trip Total\") \\\n",
    "    .alias(\"AVG Trip Total\")) \\\n",
    "    .na.drop(subset=[\"Pickup Community Area\"])\\\n",
    "    .orderBy(F.col('Pickup Community Area')).show()\n",
    "end_post = time.time()\n",
    "total_post = end_post - start_post\n",
    "print(f\"Is the dataframe cached: {df_parquet.is_cached}\")\n",
    "print(f\"Execution Time (Cached): {total_post}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ef426-2bb8-4495-ade4-504ac9356ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersisting our data\n",
    "df_parquet.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba5cd36-7cff-42c6-9a6d-cbf5f5e2dd56",
   "metadata": {},
   "source": [
    "**Bucketing**\n",
    "\n",
    "[Splitting the data into *buckets*](https://medium.com/@diehardankush/what-all-about-bucketing-and-partitioning-in-spark-bc669441db63) is another way to partition our data. In this approach, instead of partitioning the data into an n number of partitions, *buckets* can be created by partitioning the data by an specific column or set of columns. This technique is parlicularly useful when we constantly perform aggregated operations or joins. Let's take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81fedf-78d4-4481-b41c-4f0efde07a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-bucketed querying\n",
    "start = time.time()\n",
    "df_parquet.groupBy(\"Pickup Community Area\").agg({\"Trip ID\": \"count\", \"Fare\": \"avg\"}).show()\n",
    "end = time.time()\n",
    "print(f\"Time: {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657a06f-b00f-4c55-885e-75094fab58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.write.bucketBy(10, \"Pickup Community Area\").sortBy(\"Pickup Community Area\").saveAsTable(\"bucket_taxi\")\n",
    "bucketed_df = spark.table(\"bucket_taxi\")\n",
    "\n",
    "# Bucketed querying\n",
    "start = time.time()\n",
    "bucketed_df.groupBy(\"Pickup Community Area\").agg({\"Trip ID\": \"count\", \"Fare\": \"avg\"}).show()\n",
    "end = time.time()\n",
    "print(f\"Time: {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb9b90-75f8-4d24-8c1a-7469889ff522",
   "metadata": {},
   "source": [
    "As we stated at the beginning, finding the proper configuration is a task that highly depends on the type of data you're working with, as well as your available resources. \n",
    "\n",
    "In this courselet we have explored some techniques, mostly to illustrate their implementation, although some of them might not have led to an actual improvement in the performance of our cluster. You're invitated to learn more about fine tuning by reading at the relevant documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
